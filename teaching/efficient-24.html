
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Simon Ostermann</title>
        <!-- Favicon-->
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Responsive navbar-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../index.html">Simon Ostermann</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                        <li class="nav-item"><a class="nav-link active" aria-current="page" href="../index.html">Home</a></li>
                </div>
            </div>
        </nav>
        <!-- Page content-->
        <div class="container">
            <div class="text-center mt-5">
                <h3>Seminar: Efficient and Robust Natural Language Processing</h3>
                <h4>Mo 14:15–15:45, Seminarraum 1.12</h4>
                <h6>Dr. Simon Ostermann, Tatiana Anikina, Natalia Skachkova</h6>
                <h6>Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)</h6>
                </div>
            <div class="row align-items-start">
                <div class="col">

                </div>
                <div class="col-8">
                    <div class="container mt-2" style="background-color: #D19589">
                        <!--<h6>The registration to the seminar is now closed. I've dropped an email to everybody that asked to join. Many thanks for your interest! :-) </h6>-->
                        <h5>If you would like to participate, please drop an email to <i>simon.ostermann@dfki.de</i><br><strong> until April 17 (23:59)</strong>.</h5>
                        In your email, please:
                        <ul>
                            <li>Give your name, semester, study program</li>
                            <li>Write some words on why you want to take part in this course</li>
                            <li>List some of your previous experience:
                                <ul>
                                    <li>your background in <strong>deep learning or machine learning</strong></li>
                                    <li>your background in <strong>natural language processing in general</strong></li>
                                    <!--<li>your previous hands-on experience in <strong>programming/implementing NLP models</strong></li>-->
                                </ul>
                            </li>
                        </ul>

                    </div>
                    <div class="container mt-2" style="background-color: #D19589">
                        Prerequisites: This seminar is primarily targeted at Master students, but is also open to advanced Bachelor students. We expect you to have a curious mind and some familiarity with large language models. At the very least, we expect all students to have read (and understood :-)) the <a href = "https://arxiv.org/pdf/1810.04805.pdf" >BERT paper</a> and the <a href = "https://arxiv.org/pdf/1706.03762.pdf" >Transformer paper</a>.
                    </div>
                    <!--<div class="container mt-2" style="background-color: #DFA69B">
                        * <font size="-1">Don't worry if you don't have a background in all of these areas. What you should bring is a basic understanding of deep learning methods and no fear of digging into a formula, if necessary. Still, this is going to be an application-oriented seminar rather than a mathematics course.</i></font>
                    </div>
                    -->
                    <hr>
                    <div class="container mt-2" style="background-color: #e3e3e3">
                        <h4> Seminar Content </h4>
                        <p>
                           Nowadays, Large Language Models (LLMs) achieve impressive results on a variety of NLP tasks and languages. This increase in performance comes with a specific cost: Better performance is typically attributed to an increased number of parameters and training data. This means that better and larger models also need more computational resources, more time, more memory, and more energy, which makes training processes prohibitively expensive. 
                        </p>
                        <p>
                            How can we solve the problem of ever larger and ever hungrier models? Efficient NLP is an umbrella term that includes a lot of different approaches to address these problems in terms of model design and data efficiency, by introducing more efficient fine-tuning, inference, or hardware utilization. In this seminar we will mostly focus on model and data-level efficiency and will explore various approaches including efficient prompting methods, adapters, transfer learning, data augmentation and active learning. 
                        </p>

                        <hr>
                        <h4> List of relevant Papers and Topics (subject to smaller changes) </h4>
                        <h5>Data Efficiency</h5>
                        <ul>
                            <li> FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models | <a href='https://aclanthology.org/2023.emnlp-main.896/'> paper </a> </li>
                            <li> MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER | <a href='https://aclanthology.org/2022.acl-long.160/'> paper </a> </li>
                            <li> A Survey of Data Augmentation Approaches for NLP | <a href='https://aclanthology.org/2021.findings-acl.84/'> paper </a> </li>
                            <li> Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning | <a href='https://aclanthology.org/2021.eacl-main.61.pdf'> paper </a> </li>
                            <li> Applying Natural Annotation and Curriculum Learning to Named Entity Recognition for Under-Resourced Languages | <a href='https://aclanthology.org/2022.coling-1.394.pdf'> paper </a> </li>
                            <li> Do We Need to Create Big Datasets to Learn a Task? | <a href='https://aclanthology.org/2020.sustainlp-1.23/'> paper </a> </li>
                            <li> NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework | <a href='https://proceedings.mlr.press/v162/yao22c/yao22c.pdf'> paper </a> </li>
                            <li> Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes | <a href='https://arxiv.org/pdf/2305.02301.pdf?utm_referrer=https%3A%2F%2Fdzen.ru%2Fmedia%2Fid%2F5a2a9f9bfd96b1d33aaf1ec9%2F6457f09f7906327d86410618'> paper </a> </li>
                        </ul>
                        <h5>Model Efficiency</h5>
                        <ul>
                            <li> ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts  | <a href='https://aclanthology.org/2022.emnlp-main.446'> paper </a> </li>
                            <li> Parameter-Efficient Transfer Learning for NLP | <a href='https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf'> paper </a> </li>
                            <li> SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer | <a href='https://aclanthology.org/2022.acl-long.346'> paper </a> </li>
                            <li> The Power of Scale for Parameter-Efficient Prompt Tuning | <a href='https://aclanthology.org/2021.emnlp-main.243'> paper </a> </li>
                            <li> SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot | <a href='https://arxiv.org/pdf/2301.00774.pdf'> paper </a> </li>
                            <li> An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks | <a href='https://arxiv.org/pdf/2210.16773.pdf'> paper </a> </li>
                            <li> Hyperdecoders: Instance-specific decoders for multi-task NLP | <a href='https://arxiv.org/pdf/2203.08304.pdf'> paper </a> </li>
                            <li> Don’t Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner | <a href='https://proceedings.neurips.cc/paper_files/paper/2023/file/1289f9195d2ef8cfdfe5f50930c4a7c4-Paper-Conference.pdf'> paper </a> </li>
                            <li> Efficient Multimodal Fusion via Interactive Prompting | <a href='https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf'> paper </a> </li>
                            <li> HyperPrompt: Prompt-based Task-Conditioning of Transformers | <a href='https://proceedings.mlr.press/v162/he22f/he22f.pdf'> paper </a> </li>
                            <li> PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models | <a href='https://arxiv.org/pdf/2204.01172.pdf'> paper </a> </li>
                            <li> Learning to Compress Prompts with Gist Tokens | <a href='https://proceedings.neurips.cc/paper_files/paper/2023/file/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf'> paper </a> </li>
                        </ul>
                    </div>
                    <div class="container mt-2" style="background-color: #9AD5A3">
                        <font size="-1"><p>Some words on grading: This seminar is meant to be as interactive as possible. Final grades will be based on students' presentations, term papers (optional), but also on participation and discussion in class.</p>
                        <p>The participants are expected to prepare for classes accordingly, by reading the relevant papers and also doing background reading, if necessary. Based on this preparation, the participants should be able to discuss the presented papers in depth and to understand relevant context during the discussion.</p></font>
                    </div>
                    
                </div>
                <div class="col">
                    
                </div>
            </div>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../js/scripts.js"></script>
    </body>
</html>

