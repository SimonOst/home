<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Simon Ostermann</title>
        <!-- Favicon-->
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Responsive navbar-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../index.html">Simon Ostermann</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                        <li class="nav-item"><a class="nav-link active" aria-current="page" href="../index.html">Home</a></li>
                </div>
            </div>
        </nav>
        <!-- Page content-->
        <div class="container">
            <div class="text-center mt-5">
                <h3>Seminar XPLN: Exploring Explainability in NLP</h3>
                <h4>Wed 14:15–15:45, Room -1.05</h4>
                <h6>Dr. Simon Ostermann, Tanja Bäumel</h6>
                <h6>Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)</h6>
            </div>
            <div class="row align-items-start">
                <div class="col">

                </div>
                <div class="col-8">
                    <div class="container mt-2" style="background-color: #D19589">
                        <!--<h6>The registration to the seminar is now closed. I've dropped an email to everybody that asked to join. Many thanks for your interest! :-) </h6>-->
                        <h5>If you would like to participate, please drop an email to <i>xpln-seminar@dfki.de</i><br><strong> until April 17 (23:59)</strong>.</h5>
                        In your email, please:
                        <ul>
                            <li>Give your name, semester, study program</li>
                            <li>Write some words on why you want to take part in this course</li>
                            <li>List some of your previous experience:
                                <ul>
                                    <li>your background in <strong>deep learning or machine learning</strong></li>
                                    <li>your background in <strong>natural language processing in general</strong></li>
                                    <!--<li>your previous hands-on experience in <strong>programming/implementing NLP models</strong></li>-->
                                </ul>
                            </li>
                        </ul>

                    </div>
                    <div class="container mt-2" style="background-color: #D19589">
                        Prerequisites: This seminar is primarily targeted at Master students, but is also open to advanced Bachelor students. We expect you to have a curious mind and some familiarity with large language models. At the very least, we expect all students to have read (and understood :-)) the <a href = "https://arxiv.org/pdf/1810.04805.pdf" >BERT paper</a> and the <a href = "https://arxiv.org/pdf/1706.03762.pdf" >Transformer paper</a>.
                    </div>
                    <!--<div class="container mt-2" style="background-color: #DFA69B">
                        * <font size="-1">Don't worry if you don't have a background in all of these areas. What you should bring is a basic understanding of deep learning methods and no fear of digging into a formula, if necessary. Still, this is going to be an application-oriented seminar rather than a mathematics course.</i></font>
                    </div>
                    -->
                    <hr>
                    <div class="container mt-2" style="background-color: #e3e3e3">
                        <h4> Seminar Content </h4>
                        <p>
                            The rise of deep learning in AI has dramatically increased the performance of models across many sub-fields such as natural language processing or computer vision. In the last 5 years, large petrained language models (LLMs) and their variants (BERT, ChatGPT etc.) have changed the NLP landscape drastically. Such models got larger and larger over the last years, reaching  increasingly impressive performance peeks, sometimes even surpassing humans.
                        </p>
                        <p>
                            A central issue with deep learning models with millions or billions of parameters is that they are essentially <strong>black boxes</strong>: From the model's parameters, it is not inherently clear why a model exhibits a certain behavior or makes certain classification decision. Understanding the inner workings of such large models is however extremely important, especially when AI takes on critical tasks e.g. in the medical or financial domain. Trustworthiness and fairness are important dimensions that such large models should adhere to, that are often not taken into account.
                        </p>
                        <p>
                            In this seminar we will try to shine a spotlight on the rapidly growing field of interpretable and explainable AI (XAI), that develops methods to peak into the black box that LLMs are. We will introduce general methods used in XAI, and look into insights gained from applying these methods to LLMs. Depending on the students' preferences, we will cover some of the topics listed below.
                        </p>
                        <hr>
                        <h4> List of relevant Papers and Topics (subject to smaller changes) </h4>
                        <h5>Methods of XAI</h5>
                        <ul>
                            <li> <strong>Probing</strong>: What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties | <a href='https://aclanthology.org/P18-1198.pdf'> paper </a> </li>
                            <li> <strong>Amnesic Probing</strong>: Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals | <a href='https://arxiv.org/pdf/2006.00995.pdf'> paper </a> </li>
                            <li> <strong>LIME</strong>: "Why Should I Trust You?": Explaining the Predictions of Any Classifier | <a href='https://arxiv.org/pdf/1602.04938.pdf'> paper </a> </li>
                            <li> <strong>Attention Interpretation</strong>:
                                <ul>
                                    <li>Is Attention Interpretable? | <a href='https://aclanthology.org/P19-1282.pdf'> paper </a> </li>
                                    <li>Attention is not Explanation | <a href='https://aclanthology.org/N19-1357.pdf'> paper </a> </li>
                                    <li>Attention is not not Explanation | <a href='https://aclanthology.org/D19-1002.pdf'> paper </a> </li>
                                </ul>
                             <li> Axiomatic Attribution for Deep Networks | <a href='https://proceedings.mlr.press/v70/sundararajan17a.html'> paper </a> </li>

                        </ul>
                        <h5>Interpreting Language Models</h5>
                        <ul>
                            <li> <strong>Prompt Learning</strong>: Personalized Prompt Learning for Explainable Recommendation  | <a href='https://dl.acm.org/doi/abs/10.1145/3580488'> paper </a> </li>
                            <li> <strong>Syntax</strong>:
                                <ul>
                                    <li>  Open Sesame: Getting Inside BERT’s Linguistic Knowledge | <a href='https://aclanthology.org/W19-4825.pdf'> paper </a> </li>
                                    <li>  A Structural Probe for Finding Syntax in Word Representations | <a href='https://aclanthology.org/N19-1419.pdf'> paper </a> </li>
                                </ul>
                            </li>
                            <li> <strong>Semantics </strong>:
                                <ul>
                                    <li>  What do you learn from context? Probing for sentence structure in contextualized word representations | <a href='https://openreview.net/pdf?id=SJzSgnRcKX'> paper </a> </li>
                                    <li>  What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name? | <a href='https://aclanthology.org/2020.repl4nlp-1.24.pdf'> paper </a> </li>
                                </ul>
                            </li>
                            <li> <strong>(World) Knowledge</strong>:
                                <ul>
                                    <li>  Do Neural Language Representations Learn Physical Commonsense? | <a href='https://arxiv.org/pdf/1908.02899.pdf'> paper </a> </li>
                                    <li>  Language Models as Knowledge Bases? | <a href='https://arxiv.org/abs/1909.01066'> paper </a> </li>
                                    <li>  BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA | <a href='https://arxiv.org/pdf/1911.03681v1.pdf'> paper </a> </li>
                                    <li>  Do NLP Models Know Numbers? Probing Numeracy in Embeddings | <a href='https://aclanthology.org/D19-1534/'> paper </a> </li>
                                    <li>  Language Models Represent Space and Time | <a href='https://openreview.net/forum?id=jE8xbmvFin'> paper </a> </li>
                                </ul>
                            </li>
                        </ul>

                        <h5>Fairness and Bias</h5>
                        <ul>
                            <li>  Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models | <a href='https://aclanthology.org/2022.naacl-main.122.pdf'> paper </a>
                            <li>  Bigger Data or Fairer Data? Augmenting BERT via Active Sampling for Educational Text Classification | <a href='https://aclanthology.org/2022.coling-1.109.pdf'> paper </a> </li>
                            <li>  Mitigating Language-Dependent Ethnic Bias in BERT | <a href='https://aclanthology.org/2021.emnlp-main.42.pdf'> paper </a> </li>
                            <li>  Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model  | <a href='https://aclanthology.org/2023.blackboxnlp-1.29/'> paper </a> </li>
                            <li>  Investigating Gender Bias in Language Models Using Causal Mediation Analysis | <a href='https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html'> paper </a> </li>
                            </li>
                        </ul>

                        <h5>Can Models learn to Understand?</h5>
                        <ul>
                            <li>  On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? | <a href='https://dl.acm.org/doi/abs/10.1145/3442188.3445922'> paper </a> </li>
                            <li>  Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data | <a href='https://aclanthology.org/2020.acl-main.463/'> paper </a> </li>
                            <li>  On the paradox of learning to reason from data | <a href='https://dl.acm.org/doi/abs/10.24963/ijcai.2023/375'> paper </a> </li>
                            <li>  Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models | <a href='https://openreview.net/forum?id=uyTL5Bvosj&nesting=2&sort=date-desc'> paper </a> </li>
                        </ul>

                        <h5>Mechanistic Interpretability</h5>
                        <ul>
                             <li> <strong>Basics</strong>:
                                <ul>
                                    <li>  Zoom In: An Introduction to Circuits | <a href='https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes'> paper </a> </li>
                                    <li>  A Mathematical Framework for Transformer Circuits | <a href='https://transformer-circuits.pub/2021/framework/index.html'> paper </a> </li>
                                    <li>  Transformer Feed-Forward Layers Are Key-Value Memories | <a href='https://aclanthology.org/2021.emnlp-main.446/'> paper </a> </li>
                                </ul>
                            </li>
                             <li> <strong>Knowledge: Locating, Extracting, Editing & Forgetting</strong>:
                                <ul>
                                    <li>  Crawling The Internal Knowledge-Base of Language Models | <a href='https://aclanthology.org/2023.findings-eacl.139/'> paper </a> </li>
                                    <li>  Locating and Editing Factual Associations in GPT | <a href='https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html'> paper </a> </li>
                                    <li>  Mass-Editing Memory in a Transformer | <a href='https://openreview.net/forum?id=MkbcAHIYgyS'> paper </a> </li>
                                    <li>  Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models | <a href='https://openreview.net/forum?id=EldbUlZtbd'> paper </a> </li>
                                    <li>  In-Context Unlearning: Language Models as Few Shot Unlearners | <a href='https://arxiv.org/abs/2310.07579'> paper </a> </li>
                                </ul>
                             <li> <strong>World Representation</strong>:
                                <ul>
                                    <li>  Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task | <a href='https://openreview.net/forum?id=DeG07_TcZvT'> paper </a> </li>
                                    <li>  Emergent Linear Representations in World Models of Self-Supervised Sequence Models | <a href='https://aclanthology.org/2023.blackboxnlp-1.2/'> paper </a> </li>
                                </ul>
                            </li>
                        </ul>

                        <h5>Reasoning in LLMs</h5>
                        <ul>
                            <li>  A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis | <a href='https://aclanthology.org/2023.emnlp-main.435/'> paper </a> </li>
                            <li>  Reasoning with Language Model is Planning with World Model | <a href='https://aclanthology.org/2023.emnlp-main.507/'> paper </a> </li>
                            <li>  Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation | <a href='https://arxiv.org/abs/2402.03268'> paper </a> </li>
                        </ul>

                          <h5>Interpreting Model Weights</h5>
                        <ul>
                            <li> Analyzing Transformers in Embedding Space | <a href='https://virtual2023.aclweb.org/paper_P676.html'> paper </a> </li>
                            <li> Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts | <a href='https://aclanthology.org/2022.naacl-main.266/'> paper </a> </li>
                            <li> Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space | <a href='https://aclanthology.org/2022.emnlp-main.3/'> paper </a> </li>
                            <li> Backward Lens: Projecting Language Model Gradients into the Vocabulary Space | <a href='https://arxiv.org/abs/2402.12865'> paper </a> </li>
                            <li> The Hidden Space of Transformer Language Adapters | <a href='https://arxiv.org/abs/2402.13137'> paper </a> </li>
                        </ul>

                        <h5>Can Models explain Models? On the Faithfulness of Explanations</h5>
                        <ul>
                            <li>  Language models can explain neurons in language models | <a href='https://openai.com/research/language-models-can-explain-neurons-in-language-models'> paper </a>
                            <li>  Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models | <a href='https://arxiv.org/abs/2402.04614'> paper </a> </li>
                            </li>
                        </ul>

                        <h5>Interpreting Individual Neurons in NLP Models</h5>
                        <ul>
                            <li>  Knowledge Neurons in Pretrained Transformers | <a href='https://aclanthology.org/2022.acl-long.581/'> paper </a>
                            <li>  On the Pitfalls of Analyzing Individual Neurons in Language Models | <a href='https://openreview.net/forum?id=8uz0EWPQIMu'> paper </a> </li>
                            <li>  An Interpretability Illusion for BERT | <a href='https://arxiv.org/abs/2104.07143'> paper </a> </li>
                            <li>  Finding Neurons in a Haystack: Case Studies with Sparse Probing | <a href='https://openreview.net/forum?id=JYs1R9IMJr'> paper </a> </li>
                            </li>
                        </ul>

                    </div>
                    <div class="container mt-2" style="background-color: #9AD5A3">
                        <font size="-1"><p>Some words on grading: This seminar is meant to be as interactive as possible. Final grades will be based on students' presentations, term papers (optional), but also on participation and discussion in class.</p>
                        <p>The participants are expected to prepare for classes accordingly, by reading the relevant papers and also doing background reading, if necessary. Based on this preparation, the participants should be able to discuss the presented papers in depth and to understand relevant context during the discussion.</p></font>
                    </div>
                    
                </div>
                <div class="col">
                    
                </div>
            </div>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../js/scripts.js"></script>
    </body>
</html>
