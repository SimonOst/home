<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Simon Ostermann</title>
        <!-- Favicon-->
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Responsive navbar-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../index.html">Simon Ostermann</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                        <li class="nav-item"><a class="nav-link active" aria-current="page" href="../index.html">Home</a></li>
                </div>
            </div>
        </nav>
        <!-- Page content-->
        <div class="container">
            <div class="text-center mt-5">
                <h3>Seminar: Recent Advances in Mechanistic Interpretability</h3>
                <strong><h4>CHANGED: Thursday 16:15–17:45, Room "Leibniz" at DFKI</h4></strong>
                <h6>Dr. Simon Ostermann, Natalia Skachkova</h6>
                <h6>Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)</h6>
            </div>
            <div class="row align-items-start">
                <div class="col">

                </div>
                <div class="col-8">
                    <div class="container mt-2" style="background-color: #D19589">
                        <!--<h6>The registration to the seminar is now closed. I've dropped an email to everybody that asked to join. Many thanks for your interest! :-) </h6>-->
                        If you would like to participate, please drop an email to <i>mechanistic-interpretability-seminar@googlegroups.com</i><strong> until October 17 (23:59)</strong>.
                        In your email, please:
                        <ul>
                            <li>Give your name, semester, study program</li>
                            <li>Write some words on why you want to take part in this course</li>
                            <li>List some of your previous experience:
                                <ul>
                                    <li>your background in <strong>deep learning or machine learning</strong></li>
                                    <li>your background in <strong>natural language processing in general</strong></li>
                                    <!--<li>your previous hands-on experience in <strong>programming/implementing NLP models</strong></li>-->
                                </ul>
                            </li>
                        </ul>

                    </div>
                    <div class="container mt-2" style="background-color: #D19589">
                        Prerequisites: This seminar is primarily targeted at Master students, but is also open to advanced Bachelor students. We expect you to have a curious mind and advanced familiarity with large language models. At the very least, we expect all students to have read (and understood :-)) the <a href = "https://arxiv.org/pdf/1810.04805.pdf" >BERT paper</a> and the <a href = "https://arxiv.org/pdf/1706.03762.pdf" >Transformer paper</a>.
                    </div>
                    <!--<div class="container mt-2" style="background-color: #DFA69B">
                        * <font size="-1">Don't worry if you don't have a background in all of these areas. What you should bring is a basic understanding of deep learning methods and no fear of digging into a formula, if necessary. Still, this is going to be an application-oriented seminar rather than a mathematics course.</i></font>
                    </div>
                    -->
                    <hr>
                    <div class="container mt-2" style="background-color: #e3e3e3">
                        <h4> Seminar Content </h4>
                        <p>
                            The rise of deep learning in AI has dramatically increased the performance of models across many sub-fields such as natural language processing or computer vision. In the last 5 years, large pretrained language models (LLMs) and their variants (BERT, ChatGPT etc.) have changed the NLP landscape drastically. Such models got larger and larger over the last years, reaching  increasingly impressive performance peeks, sometimes even surpassing humans.
                        </p>
                        <p>
                            A central issue with deep learning models with millions or billions of parameters is that they are essentially <strong>black boxes</strong>: From the model's parameters, it is not inherently clear why a model exhibits a certain behavior or makes certain classification decision. The rapidly growing field of interpretable and explainable AI (XAI) develops methods to peek into the black box that LLMs are, trying to understand the inner workings of such large models.
                        </p>
                        <p>
                            In this seminar we will investiagte a subfield in XAI, namely Mechanistic Interpretability (MI). MI aims to understand and explain the internal workings of complex machine learning models, in particular deep neural networks, by "reverse-engineering" internal mechanisms and computations inside a network's parameters. This involves analysing how certain mechanisms in the model contribute to decisions and results. The aim is to break down the ‘black box’ nature of these models and reveal the underlying calculations and internal representations that lead to predictions or behaviours.
                        </p>
                        <p>
                            We will concentrate on a range of basic methods in MI and then focus on their applications within natural language processing.
                        </p>
                        <hr>
                        <h4> List of relevant Papers and Topics (subject to changes) </h4>
                        <h5>Mechanistic Interpretability Methods</h5>
                        <ul>
                            <li>Vig et al. (2020) – Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias | <a href='https://arxiv.org/abs/2004.12265'>paper</a></li>
                            <li>Nostalgebraist (2020) - Interpreting GPT: the logit lens | <a href='https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens'>blog post</a></li>
                            <li>Geiger et al. (2021) – Causal Abstractions of Neural Networks | <a href='https://arxiv.org/abs/2106.02997'>paper</a></li>
                            <li>Goldowsky-Dill et al. (2023) – Localizing Model Behavior with Path Patching | <a href='https://arxiv.org/abs/2304.05969'>paper</a></li>
                            <li>Conmy et al. (2023) – Towards Automated Circuit Discovery for Mechanistic Interpretability | <a href='https://arxiv.org/abs/2304.14997'>paper</a></li>
                        </ul>
                        <h5>Findings of Mechanistic Interpretability</h5>
                        <ul>
                            <li>Wang et al. (2023) – Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small | <a href='https://arxiv.org/abs/2211.00593'>paper</a></li>
                            <li>Hanna et al. (2023) – How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model | <a href='https://arxiv.org/abs/2305.00586'>paper</a></li>
                            <li>Prakash et al. (2024) – Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking | <a href='https://arxiv.org/abs/2402.14811'>paper</a></li>
                            <li>Merullo et al. (2024) – Circuit Component Reuse Across Tasks in Transformer Language Models | <a href='https://arxiv.org/abs/2310.08744'>paper</a></li>
                            <li>Nanda et al. (2023) – Progress measures for grokking via mechanistic interpretability | <a href='https://arxiv.org/abs/2301.05217'>paper</a></li>
                            <li>Geva et al. (2023) - Dissecting Recall of Factual Associations in Auto-Regressive Language Models – Geva et al. (2023) | <a href='https://arxiv.org/abs/2304.14767'>paper</a></li>
                            <li>Merullo et al. (2024) - Talking Heads: Understanding Inter-layer Communication in Transformer Language Models | <a href='https://arxiv.org/abs/2406.09519'>paper</a></li>
                            <li>Gould et al. (2024) – Successor Heads: Recurring, Interpretable Attention Heads In The Wild | <a href='https://arxiv.org/abs/2312.09230'>paper</a></li>
                            <li>McDougall et al. (2024) – Copy Suppression: Comprehensively Understanding an Attention Head | <a href='https://arxiv.org/abs/2310.04625'>paper</a></li>
                            <li>Gurnee et al. (2023) – FINDING NEURONS IN A HAYSTACK: CASE STUDIES WITH SPARSE PROBING | <a href='https://arxiv.org/pdf/2305.01610'>paper</a></li>
                            <li>Geva et al. (2022) - Transformer Feed-Forward Layers Are Key-Value Memories | <a href='https://arxiv.org/abs/2012.14913'>paper</a></li>
                            <li>Todd et al. (2024) – Function Vectors in Large Language Models | <a href='https://arxiv.org/abs/2310.15213'>paper</a></li>
                            <li>Nanda et al. (2022) – Emergent Linear Representations in World Models of Self-Supervised Sequence Models | <a href='https://arxiv.org/abs/2309.00941'> paper </a></li>
                            <li>Dai et al. (2022) - Knowledge Neurons in Pretrained Transformers | <a href="https://arxiv.org/abs/2104.08696"> paper </a> </li>
                        </ul>
                        <h5>Model Editing Editing, Knowledge Location and Extraction</h5>
                        <ul>
                            <li>Cohen et al. (2023) - Crawling The Internal Knowledge-Base of Language Models | <a href='https://arxiv.org/abs/2301.12810'> paper </a> </li>
                            <li>Meng et al. (2022) - Locating and Editing Factual Associations in GPT | <a href='https://arxiv.org/abs/2202.05262'> paper </a> </li>
                            <li>Hase et al. (2023) - Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models | <a href='https://arxiv.org/abs/2301.04213'> paper </a> </li>
                            <li>Chintam et al. (2023) - Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model | <a href='https://aclanthology.org/2023.blackboxnlp-1.29/'> paper </a> </li>
                            <li>Cohen et al. (2023) - Evaluating the Ripple Effects of Knowledge Editing in Language Models | <a href='https://arxiv.org/abs/2307.12976'> paper </a></li>
                            <li>Hartvigsen et al. (2022) - Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors | <a href='https://arxiv.org/abs/2211.11031'> paper </a> </li>
                        </ul>

                        <h5>Sparse Autoencoders and Monosemanticity</h5>
                        <ul>
                            <li>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (2023) | <a href='https://transformer-circuits.pub/2023/monosemantic-features'> blog post </a> </li>
                            <li>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet (2024) | <a href='https://transformer-circuits.pub/2024/scaling-monosemanticity/'> blog post </a> </li>
                            <li>Marks et al. (2024) - Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models | <a href='https://arxiv.org/abs/2403.19647'> paper </a> </li>
                            <li>Kissane et al. (2024) - Interpreting Attention Layer Outputs with Sparse Autoencoders | <a href='https://arxiv.org/abs/2406.17759'> paper </a> </li>
                        </ul>
                        

                    </div>
                    <div class="container mt-2" style="background-color: #9AD5A3">
                        <font size="-1"><p>Some words on grading: This seminar is meant to be as interactive as possible. Final grades will be based on students' presentations, term papers (optional), but also on participation and discussion in class.</p>
                        <p>The participants are expected to prepare for classes accordingly, by reading the relevant papers and also doing background reading, if necessary. Based on this preparation, the participants should be able to discuss the presented papers in depth and to understand relevant context during the discussion.</p></font>
                    </div>
                    
                </div>
                <div class="col">
                    
                </div>
            </div>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../js/scripts.js"></script>
    </body>
</html>
